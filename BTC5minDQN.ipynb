{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First tradingEnv class\n",
    "#todo add transaction cost\n",
    "# add MACD indicator\n",
    "# add RSI indicator\n",
    "# add SMA indicator\n",
    "# add EMA indicator\n",
    "# meerdere crypto's toevoegen ( momenteel alleen BTC)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "class TradingEnv():\n",
    "    def __init__(self, initial_investment=20000, Terminal_state= 2000):\n",
    "        #investment\n",
    "        self.initial_investment = initial_investment #initial investment \n",
    "        self.Terminal_state = Terminal_state  #terminal state, when the agent reach this state or below, the episode is over\n",
    "        self.current_investment = initial_investment #current investment in a certain state\n",
    "        self.non_invested = initial_investment #non invested money in a certain state\n",
    "        self.invested = 0\n",
    "        self.worth_history = [] #worth history in a certain episode\n",
    "        # self.invested = [{'amount': 0, 'price_investment': 0, 'timestamp' : UNIX timestamp of investment}] #invested money \n",
    "        self.invested_amount = 0\n",
    "        \n",
    "        #rewards\n",
    "        self.current_reward = 0 #current reward in a certain state\n",
    "        self.total_reward = 0   #total reward in a certain episode\n",
    "        self.reward_history = [] #reward history in a certain episode\n",
    "        self.iterations = 0\n",
    "\n",
    "        #states\n",
    "        self.current_price = [0,0,0,0] #current price in a certain state [open,high,low,close]\n",
    "        self.prev_price = np.zeros(20) #previous 20 prices to keep in memory to the state values\n",
    "        self.done = False #if the episode is over\n",
    "        self.current_unix_time = 0 #current unix time in a certain state\n",
    "        self.macd = 0 #macd indicator\n",
    "        \n",
    "        \n",
    "        #actions\n",
    "        self.action = 0 #chosen action in a certain state\n",
    "        self.action_history = [] #action history in a certain episode\n",
    "        self.action_space = np.array([0,1,2]) #action space, 0 = hold, 1 = buy, 2 = sell first in the array \n",
    "\n",
    "        #Current state\n",
    "        # self.current_state = self.get_current_state() #current state\n",
    "        self.csv_to_dataframe() #convert csv's to one dataframe\n",
    "        self.csv_index = 0 #index of the csv dataframe\n",
    "\n",
    "\n",
    "    def get_current_state(self):\n",
    "        #return current state price, and invested/non invested values as state\n",
    "        return [self.current_price[0],self.current_price[1],self.current_price[2],self.current_price[3], self.invested, self.non_invested, self.invested_amount, self.current_unix_time, self.macd,self.sma10,self.sma20]\n",
    "    # def calculate_macd(self):\n",
    "    #     #calculate macd indicator\n",
    "    #     print(\"macd\")\n",
    "    #     # print(self.df.iloc[self.csv_index - 12 : self.csv_index, 3])\n",
    "    #     # print(self.df.iloc[self.csv_index - 12 : self.csv_index, 3].ewm(span=12, adjust=False).mean())\n",
    "    #     macd = self.df.iloc[self.csv_index - 500 : self.csv_index, 3].ewm(span=500, adjust=False).mean()\n",
    "    #     macd = macd.tolist()\n",
    "    #     print('res',macd)\n",
    "    #     # print('res',type(macd.tolist()))\n",
    "    #     print(\"average macd\", sum(macd) / len(macd))\n",
    "\n",
    "        return macd\n",
    "    def reset(self, demo=False):\n",
    "        #reset the environment\n",
    "        self.worth_history = []\n",
    "        self.current_investment = self.initial_investment\n",
    "        self.current_reward = 0\n",
    "        self.total_reward = 0\n",
    "        self.reward_history = []\n",
    "        self.current_price = [0,0,0,0] #current price in a certain state [open,high,low,close]\n",
    "        self.invested_amount = 0\n",
    "        self.non_invested = self.initial_investment #non invested money in a certain state\n",
    "        self.invested = 0\n",
    "        self.worth_history = [] #worth history in a certain episode\n",
    "\n",
    "        self.prev_price = np.zeros(20)\n",
    "        self.action = 0\n",
    "        self.action_history = []\n",
    "        self.done = False\n",
    "        self.current_unix_time = 0 #current unix time in a certain state\n",
    "        self.csv_index = 0 #index of the csv dataframe\n",
    "\n",
    "\n",
    "        #begin somewhere random in csv data \n",
    "        if demo == False:\n",
    "            #last 6months as test set\n",
    "            self.csv_index = np.random.randint(0, (len(self.df)/2) + 20000)\n",
    "        else:\n",
    "            self.csv_index = len(self.df) - 262975\n",
    "        #get row csv_index from self.df\n",
    "        self.current_price = self.df.iloc[self.csv_index, 3:7].values\n",
    "        self.current_unix_time = self.df.iloc[self.csv_index, 0]\n",
    "        \n",
    "        self.macd =  self.df.iloc[self.csv_index, 10]\n",
    "        self.sma20 =  self.df.iloc[self.csv_index, 9]\n",
    "        self.sma10 =  self.df.iloc[self.csv_index, 11]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return self.get_current_state(), self.done\n",
    "\n",
    "    def step(self, action, interval = 1):\n",
    "        self.iterations += 1\n",
    "        self.action = action\n",
    "        self.action_history.append(self.action)\n",
    "        \n",
    "        #calculate current reward\n",
    "        self.current_reward = self.get_reward()\n",
    "        self.done = self.is_done()\n",
    "        #if the episode is over, give a big negative reward\n",
    "        if self.done == True:\n",
    "            self.current.reward - 1000000\n",
    "\n",
    "        self.total_reward += self.current_reward\n",
    "        self.reward_history.append(self.current_reward)\n",
    "\n",
    "        #get next line of data in csv\n",
    "        if self.csv_index < len(self.df) - 1:\n",
    "            try:\n",
    "                self.csv_index += interval\n",
    "                self.current_price = self.df.iloc[self.csv_index, 3:7].values\n",
    "                self.current_unix_time = self.df.iloc[self.csv_index, 0]\n",
    "                self.macd =  self.df.iloc[self.csv_index, 10]\n",
    "                self.sma20 =  self.df.iloc[self.csv_index, 9]\n",
    "                self.sma10 =  self.df.iloc[self.csv_index, 11]\n",
    "            except Exception as ex:\n",
    "                print(\"ex: \", ex)\n",
    "                print(\"csv_index: \", self.csv_index)\n",
    "                self.done = True\n",
    "                \n",
    "\n",
    "        else:\n",
    "            self.done = True\n",
    "        return self.get_current_state(), self.current_reward, self.done\n",
    "        \n",
    "    \n",
    "    def get_reward(self):\n",
    "        #reward function\n",
    "        if self.action == 0:\n",
    "            #hold do nothing\n",
    "            reward = 1\n",
    "        elif self.action == 1:\n",
    "            #buy = invest 1/10 of non_invested money\n",
    "            to_invest = self.non_invested / 10\n",
    "            self.non_invested -= to_invest\n",
    "            amount = to_invest / self.current_price[0]\n",
    "            self.invested += to_invest\n",
    "            self.invested_amount += amount\n",
    "            reward = 1\n",
    "        elif self.action == 2:\n",
    "            #sell first bought in array of invested\n",
    "            if self.invested > 0:\n",
    "                amount = self.invested_amount\n",
    "                price_investment = self.invested\n",
    "                self.non_invested += amount * self.current_price[0]\n",
    "                self.invested_amount = 0\n",
    "                self.invested = 0\n",
    "\n",
    "                reward = (amount * self.current_price[0]) - price_investment\n",
    "            else:\n",
    "                #hold, do nothing, nothing to sell\n",
    "                reward = 1\n",
    "        \n",
    "        self.worth_history.append(self.calc_current_worth())\n",
    "        return reward * ((self.calc_current_worth()/self.initial_investment)**2)\n",
    "\n",
    "\n",
    "    def is_done(self):\n",
    "        #terminal state\n",
    "        if self.calc_current_worth() <= self.Terminal_state:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def calc_current_worth(self):\n",
    "        #calculate current worth\n",
    "        return self.invested_amount * self.current_price[0] + self.non_invested\n",
    "\n",
    "        \n",
    "    def csv_to_dataframe(self):\n",
    "        #convert csv's to one dataframe\n",
    "        df2017 = pd.read_csv('./data/BTC-2017min.csv')\n",
    "        df2018 = pd.read_csv('./data/BTC-2018min.csv')\n",
    "        df2019 = pd.read_csv('./data/BTC-2019min.csv')\n",
    "        df2020 = pd.read_csv('./data/BTC-2020min.csv')\n",
    "        df2021 = pd.read_csv('./data/BTC-2021min.csv')\n",
    "\n",
    "        #concat all dataframes\n",
    "        df = pd.concat([df2017, df2018, df2019, df2020, df2021], ignore_index=True)\n",
    "        #add new column named macd, that is the mean of the last 12 open values\n",
    "        df['sma20000'] = df['open'].rolling(window=20000).mean()\n",
    "        self.df = df\n",
    "        df['macd20000'] = df['open'].rolling(window=20000).mean()**2\n",
    "        self.df = df\n",
    "        df['sma10000'] = df['open'].rolling(window=20000).mean()\n",
    "        self.df = df\n",
    "        #len = 2675301"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cedri\\anaconda3\\envs\\ResearchProj\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "#DQN \n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "\n",
    "# Import Tensorflow libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "###### Tensorflow-GPU ########\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cedri\\anaconda3\\envs\\ResearchProj\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "#DQN \n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "\n",
    "# Import Tensorflow libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "###### Tensorflow-GPU ########\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cedri\\anaconda3\\envs\\ResearchProj\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape\n",
      "(11,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class DQAgent:\n",
    "\n",
    "    def __init__(self, replayCapacity, inputShape):\n",
    "        ## Initialize replay memory\n",
    "        self.capacity = replayCapacity\n",
    "        self.memory = collections.deque(maxlen=self.capacity)\n",
    "        self.populated = False\n",
    "        ## q network\n",
    "        self.inputShape = inputShape\n",
    "        self.q_model = self.buildNetwork()\n",
    "\n",
    "        ## Target network\n",
    "        self.target_model = self.buildNetwork()\n",
    "        self.target_model.set_weights(self.q_model.get_weights())\n",
    "\n",
    "    def addToReplayMemory(self, step):\n",
    "        self.step = step\n",
    "        self.memory.append(self.step)\n",
    "\n",
    "    def sampleFromReplayMemory(self, batchSize):\n",
    "        self.batchSize = batchSize\n",
    "        if self.batchSize > len(self.memory):\n",
    "            self.populated = False\n",
    "            return self.populated\n",
    "        else:\n",
    "            return random.sample(self.memory, self.batchSize)\n",
    "\n",
    "\n",
    "    def buildNetwork(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_shape=self.inputShape, activation='relu'))\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(Dense(3, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr = 0.001), metrics=['MeanSquaredError'])\n",
    "        return model\n",
    "\n",
    "    def q_network_fit(self,batch, batchSize):\n",
    "        self.batchSize = batchSize\n",
    "        self.batch = batch\n",
    "\n",
    "\n",
    "    def q_network_predict(self, state):\n",
    "        self.state = state\n",
    "        self.qPolicy = self.q_model.predict(self.state, verbose = 0)\n",
    "        return self.qPolicy\n",
    "\n",
    "    def target_network_predict(self, state):\n",
    "        self.state = state\n",
    "        self.qTarget = self.target_model.predict(self.state, verbose = 0)\n",
    "        return self.qTarget\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_model.set_weights(self.q_model.get_weights())\n",
    "\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "\n",
    "DISCOUNT = 0.90\n",
    "REPLAY_MEMORY_CAPACITY = 5000\n",
    "#MIN_REPLAY_MEMORY_SIZE = 1_000  # Minimum number of steps in a memory to start training\n",
    "BATCH_SIZE = 64  # How many steps (samples) to use for training\n",
    "UPDATE_TARGET_INTERVAL = 1000\n",
    "EPSILON = 0.5 # Exploration percentage\n",
    "MIN_EPSILON = 0.01\n",
    "POSSIBLE_ACTIONS = [0,1,2]\n",
    "DECAY = 0.999\n",
    "\n",
    "\n",
    "\n",
    "# Create Cartpole environment\n",
    "env = TradingEnv()\n",
    "state, done = env.reset()\n",
    "\n",
    "#state = env.reset()\n",
    "\n",
    "# create DQN agent\n",
    "\n",
    "agent = DQAgent(replayCapacity= REPLAY_MEMORY_CAPACITY, inputShape= (11,))\n",
    "# agent.q_model = keras.models.load_model('mountaincar')\n",
    "# agent.update_target_network()\n",
    "\n",
    "# Fill the replay memory with the first batch of samples\n",
    "\n",
    "updateCounter = 0\n",
    "rewardHistory = []\n",
    "\n",
    "\n",
    "score_history = []\n",
    "    \n",
    "steps = []\n",
    "\n",
    "num_episodes = 10\n",
    "fullcounter = 0\n",
    "maxIterations = 600\n",
    "allEpisodesWorth = []\n",
    "allEpisodesPrice = []\n",
    "AllEpisodesSold = []\n",
    "AllEpisodesBought = []\n",
    "\n",
    "\n",
    "for episode in range(2):\n",
    "    episodeReward = 0\n",
    "    stepCounter = 0  # count the number of successful steps within the episode\n",
    "\n",
    "    #print('\\n', episode)\n",
    "    state, done = env.reset()\n",
    "    #state = np.expand_dims(state, axis=0)\n",
    "    worth_history = []\n",
    "    price_history = []\n",
    "    bought = []\n",
    "    sold = []\n",
    "    done = False\n",
    "    score = 0\n",
    "    state = np.asarray(state)\n",
    "    print(\"shape\")\n",
    "    print(state.shape)\n",
    "    counter = 0\n",
    "    lastBuy = 0\n",
    "    stop = False\n",
    "    while not done or stop == True:\n",
    "        counter+=1\n",
    "        fullcounter += 1\n",
    "\n",
    "        # if counter%10== 0:\n",
    "            # env.render()\n",
    "\n",
    "        r = random.random()\n",
    "\n",
    "        if r <= EPSILON:\n",
    "            action = random.sample(POSSIBLE_ACTIONS, 1)[0]\n",
    "            #print('exploration')\n",
    "        else:\n",
    "            #print('exploitation')\n",
    "            qValues = agent.q_network_predict(state.reshape(1,-1))\n",
    "            action = np.argmax(qValues[0] )\n",
    "            #print('action =', action)\n",
    "            #print(qValues)\n",
    "        if env.invested < 10:\n",
    "            action = 0\n",
    "        if counter - lastBuy > 7000:\n",
    "            action = 1\n",
    "        if action == 1:\n",
    "            if counter - lastBuy > 100:\n",
    "                lastBuy = counter\n",
    "                bought.append(counter-1)\n",
    "            else:\n",
    "                action = 0\n",
    "        elif action == 2:\n",
    "            if counter - lastBuy > 10:\n",
    "                sold.append(counter-1)\n",
    "            else:\n",
    "                action = 0\n",
    "        if counter > maxIterations:\n",
    "            action = 2\n",
    "            sold.append(counter-1)\n",
    "            stop = True\n",
    "\n",
    "        steps.append(action)\n",
    "        newState, reward, done = env.step(action)\n",
    "        newState = np.asarray(newState)\n",
    "\n",
    "        # print(\"gegevens\")\n",
    "        # print(newState)\n",
    "        # print(info)\n",
    "        \n",
    "        \n",
    "\n",
    "       \n",
    "\n",
    "        #newState = np.expand_dims(newState, axis=0)\n",
    "        # store step in replay memory\n",
    "        step = (state, action, reward, newState, done)\n",
    "        agent.addToReplayMemory(step)\n",
    "        state = newState\n",
    "        episodeReward += reward\n",
    "        #print('episodeReward = ',episodeReward)\n",
    "        # When enough steps in replay memory -> train policy network\n",
    "        if len(agent.memory) >= (BATCH_SIZE ):\n",
    "            EPSILON = DECAY * EPSILON\n",
    "            if EPSILON < MIN_EPSILON:\n",
    "                EPSILON = MIN_EPSILON\n",
    "            # sample minibatch from replay memory\n",
    "            miniBatch = agent.sampleFromReplayMemory(BATCH_SIZE)\n",
    "            miniBatch_states = np.asarray(list(zip(*miniBatch))[0],dtype=float)\n",
    "            miniBatch_actions = np.asarray(list(zip(*miniBatch))[1], dtype = int)\n",
    "            miniBatch_rewards = np.asarray(list(zip(*miniBatch))[2], dtype = float)\n",
    "            miniBatch_next_state = np.asarray(list(zip(*miniBatch))[3],dtype=float)\n",
    "            miniBatch_done = np.asarray(list(zip(*miniBatch))[4],dtype=bool)\n",
    "\n",
    "            current_state_q_values = agent.q_network_predict(miniBatch_states)\n",
    "            y = current_state_q_values\n",
    "            #print(y.shape)\n",
    "            #miniBatch_next_state = np.squeeze(miniBatch_next_state, axis =1)\n",
    "\n",
    "            next_state_q_values = agent.target_network_predict(miniBatch_next_state)\n",
    "            #print(next_state_q_values.shape)\n",
    "            max_q_next_state = np.max(next_state_q_values,axis=1)\n",
    "\n",
    "            for i in range(BATCH_SIZE):\n",
    "                if miniBatch_done[i]:\n",
    "                    y[i,miniBatch_actions[i]] = miniBatch_rewards[i]\n",
    "                else:\n",
    "                    y[i,miniBatch_actions[i]] = miniBatch_rewards[i] + DISCOUNT * max_q_next_state[i]\n",
    "\n",
    "            agent.q_model.fit(miniBatch_states, y, batch_size=BATCH_SIZE, verbose = 0)\n",
    "            #print(y)\n",
    "\n",
    "        else:\n",
    "            # if counter%10== 0:\n",
    "\n",
    "                # env.render()\n",
    "            continue\n",
    "        if updateCounter == UPDATE_TARGET_INTERVAL:\n",
    "            # agent.q_model.save('mountaincar')\n",
    "            agent.update_target_network()\n",
    "            # print('target updated')\n",
    "            updateCounter = 0\n",
    "        updateCounter += 1\n",
    "    print('episodeReward for episode ', episode, '= ', episodeReward, 'with epsilon = ', EPSILON)\n",
    "    allEpisodesWorth.append(worth_history)\n",
    "    allEpisodesPrice.append(price_history)\n",
    "    AllEpisodesSold.append(sold)\n",
    "    AllEpisodesBought.append(bought)\n",
    "    rewardHistory.append(episodeReward)\n",
    "\n",
    "env.close()\n",
    "\n",
    "plt.plot(rewardHistory)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#actions = agent.policy_network_predict(state)\n",
    "\n",
    "#action = np.argmax(actions)\n",
    "#print(action)\n",
    "\n",
    "#state, reward, done, info = env.step(action)\n",
    "#print(reward)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ResearchProj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eff080227ac678accb1f47f22e9c228faa9b0afa218e8070dc42d5c73fc603f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
