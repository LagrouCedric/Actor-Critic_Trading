{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First tradingEnv class\n",
    "#todo add transaction cost\n",
    "# add MACD indicator\n",
    "# add RSI indicator\n",
    "# add SMA indicator\n",
    "# add EMA indicator\n",
    "# meerdere crypto's toevoegen ( momenteel alleen BTC)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "class TradingEnv():\n",
    "    def __init__(self, initial_investment=20000, Terminal_state= 2000):\n",
    "        #investment\n",
    "        self.initial_investment = initial_investment #initial investment \n",
    "        self.Terminal_state = Terminal_state  #terminal state, when the agent reach this state or below, the episode is over\n",
    "        self.current_investment = initial_investment #current investment in a certain state\n",
    "        self.non_invested = initial_investment #non invested money in a certain state\n",
    "        self.invested = 0\n",
    "        self.worth_history = [] #worth history in a certain episode\n",
    "        # self.invested = [{'amount': 0, 'price_investment': 0, 'timestamp' : UNIX timestamp of investment}] #invested money \n",
    "        self.invested_amount = 0\n",
    "        \n",
    "        #rewards\n",
    "        self.current_reward = 0 #current reward in a certain state\n",
    "        self.total_reward = 0   #total reward in a certain episode\n",
    "        self.reward_history = [] #reward history in a certain episode\n",
    "        self.iterations = 0\n",
    "\n",
    "        #states\n",
    "        self.current_price = [0,0,0,0] #current price in a certain state [open,high,low,close]\n",
    "        self.prev_price = np.zeros(20) #previous 20 prices to keep in memory to the state values\n",
    "        self.done = False #if the episode is over\n",
    "        self.current_unix_time = 0 #current unix time in a certain state\n",
    "        self.macd = 0 #macd indicator\n",
    "        \n",
    "        \n",
    "        #actions\n",
    "        self.action = 0 #chosen action in a certain state\n",
    "        self.action_history = [] #action history in a certain episode\n",
    "        self.action_space = np.array([0,1,2]) #action space, 0 = hold, 1 = buy, 2 = sell first in the array \n",
    "\n",
    "        #Current state\n",
    "        # self.current_state = self.get_current_state() #current state\n",
    "        self.csv_to_dataframe() #convert csv's to one dataframe\n",
    "        self.csv_index = 0 #index of the csv dataframe\n",
    "        self.demo = False\n",
    "\n",
    "    def get_current_state(self):\n",
    "        #return current state price, and invested/non invested values as state\n",
    "        return [self.current_price[0],self.current_price[1],self.current_price[2],self.current_price[3], self.invested, self.non_invested, self.invested_amount, self.current_unix_time, self.macd,self.sma10,self.sma20, self.counter, self.lastBuy, self.lastSold, self.lastBuyPrice, self.lastSoldPrice]\n",
    "    # def calculate_macd(self):\n",
    "    #     #calculate macd indicator\n",
    "    #     print(\"macd\")\n",
    "    #     # print(self.df.iloc[self.csv_index - 12 : self.csv_index, 3])\n",
    "    #     # print(self.df.iloc[self.csv_index - 12 : self.csv_index, 3].ewm(span=12, adjust=False).mean())\n",
    "    #     macd = self.df.iloc[self.csv_index - 500 : self.csv_index, 3].ewm(span=500, adjust=False).mean()\n",
    "    #     macd = macd.tolist()\n",
    "    #     print('res',macd)\n",
    "    #     # print('res',type(macd.tolist()))\n",
    "    #     print(\"average macd\", sum(macd) / len(macd))\n",
    "\n",
    "        return macd\n",
    "    def reset(self, demo=False):\n",
    "        # self.lastBuy, self.lastSold, self.lastBuyPrice, self.lastSoldPrice\n",
    "        self.lastBuy = 0\n",
    "        self.lastSold = 0\n",
    "        self.lastBuyPrice = 0\n",
    "        self.lastSoldPrice = 0\n",
    "        \n",
    "        self.counter = 0\n",
    "        #reset the environment\n",
    "        self.worth_history = []\n",
    "        self.current_investment = self.initial_investment\n",
    "        self.current_reward = 0\n",
    "        self.total_reward = 0\n",
    "        self.reward_history = []\n",
    "        self.current_price = [0,0,0,0] #current price in a certain state [open,high,low,close]\n",
    "        self.invested_amount = 0\n",
    "        self.non_invested = self.initial_investment #non invested money in a certain state\n",
    "        self.invested = 0\n",
    "        self.worth_history = [] #worth history in a certain episode\n",
    "\n",
    "        self.prev_price = np.zeros(20)\n",
    "        self.action = 0\n",
    "        self.action_history = []\n",
    "        self.done = False\n",
    "        self.current_unix_time = 0 #current unix time in a certain state\n",
    "        self.csv_index = 0 #index of the csv dataframe\n",
    "        self.demo = demo\n",
    "\n",
    "        #begin somewhere random in csv data \n",
    "        \n",
    "        if demo == False:\n",
    "            #last 6months as test set\n",
    "            self.csv_index = np.random.randint(0, (len(self.df)/2) + 40000)\n",
    "        else:\n",
    "            self.csv_index = len(self.df) - 262975\n",
    "        #get row csv_index from self.df\n",
    "        self.current_price = self.df.iloc[self.csv_index, 3:7].values\n",
    "        self.current_unix_time = self.df.iloc[self.csv_index, 0]\n",
    "        \n",
    "        self.macd =  self.df.iloc[self.csv_index, 10]\n",
    "        self.sma20 =  self.df.iloc[self.csv_index, 9]\n",
    "        self.sma10 =  self.df.iloc[self.csv_index, 11]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return self.get_current_state(), self.done\n",
    "\n",
    "    def step(self, action, interval = 1):\n",
    "        self.counter +=1\n",
    "        self.iterations += 1\n",
    "        self.action = action\n",
    "        self.action_history.append(self.action)\n",
    "        \n",
    "        #calculate current reward\n",
    "        self.current_reward = self.get_reward()\n",
    "        self.done = self.is_done()\n",
    "        #if the episode is over, give a big negative reward\n",
    "        if self.done == True:\n",
    "            self.current.reward - 1000000\n",
    "\n",
    "        self.total_reward += self.current_reward\n",
    "        self.reward_history.append(self.current_reward)\n",
    "\n",
    "        #get next line of data in csv\n",
    "        if self.csv_index < len(self.df) - 1:\n",
    "            self.csv_index += interval\n",
    "            self.current_price = self.df.iloc[self.csv_index, 3:7].values\n",
    "            self.current_unix_time = self.df.iloc[self.csv_index, 0]\n",
    "            self.macd =  self.df.iloc[self.csv_index, 10]\n",
    "            self.sma20 =  self.df.iloc[self.csv_index, 9]\n",
    "            self.sma10 =  self.df.iloc[self.csv_index, 11]\n",
    "\n",
    "        else:\n",
    "            self.done = True\n",
    "        if self.csv_index > len(self.df) - 262975 and self.demo == False:\n",
    "            self.done = True\n",
    "        \n",
    "        return self.get_current_state(), self.current_reward, self.done\n",
    "        \n",
    "    def get_reward(self):\n",
    "        #reward function\n",
    "        if self.action == 0:\n",
    "            #hold do nothing\n",
    "            reward = .1\n",
    "        elif self.action == 1:\n",
    "    \n",
    "            #buy = invest 1/10 of non_invested money\n",
    "            to_invest = self.non_invested / 10\n",
    "            self.non_invested -= to_invest\n",
    "            amount = to_invest / self.current_price[0]\n",
    "            self.invested += to_invest\n",
    "            self.invested_amount += amount\n",
    "            self.lastBuy = self.counter\n",
    "            self.lastBuyPrice = self.current_price[0]\n",
    "            reward = .1\n",
    "        elif self.action == 2:\n",
    "        \n",
    "            amount = self.invested_amount\n",
    "            price_investment = self.invested\n",
    "            self.non_invested += amount * self.current_price[0]\n",
    "            self.invested_amount = 0\n",
    "            self.invested = 0\n",
    "            self.lastSold = self.counter\n",
    "            self.lastSoldPrice = self.current_price[0]\n",
    "\n",
    "            reward = ((amount * self.current_price[0]) - price_investment)**3\n",
    "           \n",
    "        \n",
    "        self.worth_history.append(self.calc_current_worth())\n",
    "        if reward > 0: \n",
    "            return (reward**2) * ((self.calc_current_worth()/self.initial_investment)**2)\n",
    "        else:\n",
    "            return (reward**3) * ((self.calc_current_worth()/self.initial_investment)**2)\n",
    "\n",
    "\n",
    "\n",
    "    def is_done(self):\n",
    "        #terminal state\n",
    "        if self.calc_current_worth() <= self.Terminal_state:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def calc_current_worth(self):\n",
    "        #calculate current worth\n",
    "        return self.invested_amount * self.current_price[0] + self.non_invested\n",
    "\n",
    "        \n",
    "    def csv_to_dataframe(self):\n",
    "        #convert csv's to one dataframe\n",
    "        df2017 = pd.read_csv('../data/BTC-2017min.csv')\n",
    "        df2018 = pd.read_csv('../data/BTC-2018min.csv')\n",
    "        df2019 = pd.read_csv('../data/BTC-2019min.csv')\n",
    "        df2020 = pd.read_csv('../data/BTC-2020min.csv')\n",
    "        df2021 = pd.read_csv('../data/BTC-2021min.csv')\n",
    "\n",
    "        #concat all dataframes\n",
    "        df = pd.concat([df2017, df2018, df2019, df2020, df2021], ignore_index=True)\n",
    "        #add new column named macd, that is the mean of the last 12 open values\n",
    "        df['sma40000'] = df['open'].rolling(window=40000).mean()\n",
    "        self.df = df\n",
    "        df['macd20000'] = df['open'].rolling(window=20000).mean()**2\n",
    "        self.df = df\n",
    "        df['sma10000'] = df['open'].rolling(window=20000).mean()\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True) \n",
    "        df.dropna(inplace=True) \n",
    "        self.df = df\n",
    "        #len = 2675301"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000.0 0.010000000000000002\n",
      "3800.0 0.010002724981253113\n",
      "5420.0 0.010026205011426747\n",
      "invested:  5420.0 non_invested:  14580.0 worth:  20032.38127703651\n",
      "position size:  2.43\n",
      "[6840.0, 6840.0, 6840.0, 6840.0, 6840.0]\n"
     ]
    }
   ],
   "source": [
    "class GridTrading:\n",
    "    def __init__(self, current_price) -> None:\n",
    "        self.line_count = 5\n",
    "        self.grid_space = 100\n",
    "        self.current_price = current_price\n",
    "    \n",
    "    def calculate_grid_lines(self):\n",
    "        line_count = self.line_count\n",
    "        grid_space = self.grid_space\n",
    "        price = self.current_price\n",
    "        \n",
    "        stop_loss = price - ((line_count + 1) * grid_space)\n",
    "        take_profit = price + ((line_count + 1) * grid_space)\n",
    "        buy_lines = []\n",
    "        sell_lines = []\n",
    "     \n",
    "        for i in range(line_count):\n",
    "             buy_lines.append(price - ((line_count * grid_space)))\n",
    "             sell_lines.append(price + ((line_count * grid_space)))\n",
    "         \n",
    "        return stop_loss, take_profit, buy_lines, sell_lines\n",
    "    \n",
    "    # risk no more than 10 percent of our account on a single trade\n",
    "    # when the stoploss is triggered we should only lose 10 percent\n",
    "    def calculate_position_size(self, account_size):\n",
    "        line_count = self.line_count\n",
    "        grid_space = self.grid_space\n",
    "        \n",
    "        risk = account_size * .1\n",
    "        tick_value = 1\n",
    "        ticks_at_risk = (line_count + 1) * grid_space\n",
    "        position_size = risk / (ticks_at_risk * tick_value)\n",
    "        position_size = round(position_size, 2)\n",
    "        return position_size\n",
    "\n",
    "    \n",
    "\n",
    "env = TradingEnv()\n",
    "iterations = 1000\n",
    "sold = []\n",
    "bought = []\n",
    "worth = []\n",
    "price = []\n",
    "state, done = env.reset()\n",
    "gridTrading = GridTrading(env.current_price[0])\n",
    "counter = 0\n",
    "for i in range(3):\n",
    "    #Buy 3 times at start to have some amount to work with\n",
    "    state, reward, done = env.step(1)\n",
    "    counter += 1\n",
    "    print(env.invested, reward)\n",
    "print(\"invested: \", env.invested, \"non_invested: \", env.non_invested, \"worth: \", env.calc_current_worth())\n",
    "pos_size = gridTrading.calculate_position_size(env.non_invested)\n",
    "print(\"position size: \", pos_size)\n",
    "\n",
    "stop_loss, take_profit, buy_lines, sell_lines = gridTrading.calculate_grid_lines()\n",
    "\n",
    "for i in range(iterations):\n",
    "    if state[0] in buy_lines:\n",
    "        print(\"buy\")\n",
    "        state, reward, done = env.step(1)\n",
    "        bought.append(counter)\n",
    "        # print(env.invested, reward)\n",
    "    elif state[0] in sell_lines:\n",
    "        print(\"sell\")\n",
    "        sold.append(counter)\n",
    "        state, reward, done = env.step(3)\n",
    "        # print(env.invested, reward)\n",
    "    elif take_profit == state[0]:\n",
    "        print(\"sell\")\n",
    "        # sold.append(counter)\n",
    "\n",
    "        state, reward, done = env.step(2)\n",
    "        print(env.invested, reward)\n",
    "    elif stop_loss == state[0]:\n",
    "        print(\"sell\")\n",
    "        sold.append(counter)\n",
    "\n",
    "        state, reward, done = env.step(2)\n",
    "        # print(env.invested, reward)\n",
    "    else:\n",
    "        state, reward, done = env.step(0)\n",
    "        # print(env.invested, reward)\n",
    "\n",
    "    counter += 1\n",
    "\n",
    "#\n",
    "print(buy_lines)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ResearchProj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eff080227ac678accb1f47f22e9c228faa9b0afa218e8070dc42d5c73fc603f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
