{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First tradingEnv class\n",
    "#todo add transaction cost\n",
    "# add MACD indicator\n",
    "# add RSI indicator\n",
    "# add SMA indicator\n",
    "# add EMA indicator\n",
    "# meerdere crypto's toevoegen ( momenteel alleen BTC)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "class TradingEnv():\n",
    "    def __init__(self, initial_investment=20000, Terminal_state= 10000):\n",
    "        #investment\n",
    "        self.initial_investment = initial_investment #initial investment \n",
    "        self.Terminal_state = Terminal_state  #terminal state, when the agent reach this state or below, the episode is over\n",
    "        self.current_investment = initial_investment #current investment in a certain state\n",
    "        self.non_invested = initial_investment #non invested money in a certain state\n",
    "        self.invested = 0\n",
    "        self.worth_history = [] #worth history in a certain episode\n",
    "        # self.invested = [{'amount': 0, 'price_investment': 0, 'timestamp' : UNIX timestamp of investment}] #invested money \n",
    "        self.invested_amount = 0\n",
    "        \n",
    "        #rewards\n",
    "        self.current_reward = 0 #current reward in a certain state\n",
    "        self.total_reward = 0   #total reward in a certain episode\n",
    "        self.reward_history = [] #reward history in a certain episode\n",
    "        self.iterations = 0\n",
    "\n",
    "        #states\n",
    "        self.current_price = [0,0,0,0] #current price in a certain state [open,high,low,close]\n",
    "        self.prev_price = np.zeros(20) #previous 20 prices to keep in memory to the state values\n",
    "        self.done = False #if the episode is over\n",
    "        self.current_unix_time = 0 #current unix time in a certain state\n",
    "        self.macd = 0 #macd indicator\n",
    "        \n",
    "        \n",
    "        #actions\n",
    "        self.action = 0 #chosen action in a certain state\n",
    "        self.action_history = [] #action history in a certain episode\n",
    "        self.action_space = np.array([0,1,2]) #action space, 0 = hold, 1 = buy, 2 = sell first in the array \n",
    "\n",
    "        #Current state\n",
    "        # self.current_state = self.get_current_state() #current state\n",
    "        self.csv_to_dataframe() #convert csv's to one dataframe\n",
    "        self.csv_index = 0 #index of the csv dataframe\n",
    "        self.demo = False\n",
    "\n",
    "    def get_current_state(self):\n",
    "        #return current state price, and invested/non invested values as state\n",
    "        return [self.current_price[0],self.current_price[1],self.current_price[2],self.current_price[3], self.invested, self.non_invested, self.invested_amount, self.macd,self.sma30,self.sma60, self.lastBuy, self.lastSold, self.lastBuyPrice, self.lastSoldPrice]\n",
    "    # def calculate_macd(self):\n",
    "    #     #calculate macd indicator\n",
    "    #     print(\"macd\")\n",
    "    #     # print(self.df.iloc[self.csv_index - 12 : self.csv_index, 3])\n",
    "    #     # print(self.df.iloc[self.csv_index - 12 : self.csv_index, 3].ewm(span=12, adjust=False).mean())\n",
    "    #     macd = self.df.iloc[self.csv_index - 500 : self.csv_index, 3].ewm(span=500, adjust=False).mean()\n",
    "    #     macd = macd.tolist()\n",
    "    #     print('res',macd)\n",
    "    #     # print('res',type(macd.tolist()))\n",
    "    #     print(\"average macd\", sum(macd) / len(macd))\n",
    "\n",
    "        return macd\n",
    "    def reset(self, demo=False):\n",
    "        # self.lastBuy, self.lastSold, self.lastBuyPrice, self.lastSoldPrice\n",
    "        self.lastBuy = -9999\n",
    "        self.lastSold = -9999\n",
    "        self.lastBuyPrice = 0\n",
    "        self.lastSoldPrice = 0\n",
    "        \n",
    "        self.counter = 0\n",
    "        #reset the environment\n",
    "        self.worth_history = []\n",
    "        self.current_investment = self.initial_investment\n",
    "        self.current_reward = 0\n",
    "        self.total_reward = 0\n",
    "        self.reward_history = []\n",
    "        self.current_price = [0,0,0,0] #current price in a certain state [open,high,low,close]\n",
    "        self.invested_amount = 0\n",
    "        self.non_invested = self.initial_investment #non invested money in a certain state\n",
    "        self.invested = 0\n",
    "        self.worth_history = [] #worth history in a certain episode\n",
    "\n",
    "        self.prev_price = np.zeros(20)\n",
    "        self.action = 0\n",
    "        self.action_history = []\n",
    "        self.done = False\n",
    "        self.current_unix_time = 0 #current unix time in a certain state\n",
    "        self.csv_index = 0 #index of the csv dataframe\n",
    "        self.demo = demo\n",
    "\n",
    "        #begin somewhere random in csv data \n",
    "        \n",
    "        if demo == False:\n",
    "            #last 6months as test set\n",
    "            self.csv_index = 2100000\n",
    "        else:\n",
    "            self.csv_index = len(self.df) - 262975\n",
    "        #get row csv_index from self.df\n",
    "        self.current_price = self.df.iloc[self.csv_index, 1:5].values\n",
    "        self.current_unix_time = self.df.iloc[self.csv_index, 0]\n",
    "        \n",
    "        self.macd =  self.df.iloc[self.csv_index, -1]\n",
    "        self.sma60 =  self.df.iloc[self.csv_index, -5]\n",
    "        self.sma30 =  self.df.iloc[self.csv_index, -4]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        return self.get_current_state(), self.done\n",
    "\n",
    "    def step(self, action, interval = 1):\n",
    "        self.counter +=1\n",
    "        self.lastBuy -= 1\n",
    "        self.lastSold -= 1\n",
    "        self.iterations += 1\n",
    "        self.action = action\n",
    "        self.action_history.append(self.action)\n",
    "        \n",
    "        #calculate current reward\n",
    "        self.current_reward = self.get_reward()\n",
    "        self.done = self.is_done()\n",
    "        #if the episode is over, give a big negative reward\n",
    "        if self.done == True:\n",
    "            self.current_reward - 10000000\n",
    "\n",
    "        self.total_reward += self.current_reward\n",
    "        self.reward_history.append(self.current_reward)\n",
    "\n",
    "        #get next line of data in csv\n",
    "        if self.csv_index < len(self.df) - 100:\n",
    "            self.csv_index += interval\n",
    "            self.current_price = self.df.iloc[self.csv_index, 1:5].values\n",
    "\n",
    "            self.current_unix_time = self.df.iloc[self.csv_index, 0]\n",
    "            self.macd =  self.df.iloc[self.csv_index, -1]\n",
    "            self.sma60 =  self.df.iloc[self.csv_index, -5]\n",
    "            self.sma30 =  self.df.iloc[self.csv_index, -4]\n",
    "\n",
    "        else:\n",
    "            self.done = True\n",
    "        # if self.csv_index > len(self.df) - 262975 and self.demo == False:\n",
    "        #     self.done = True\n",
    "        #     print(\"end of DF\")\n",
    "        \n",
    "        return self.get_current_state(), self.current_reward, self.done\n",
    "        \n",
    "    def get_reward(self):\n",
    "        #reward function\n",
    "        if self.action == 0:\n",
    "            #hold do nothing\n",
    "            return -5\n",
    "        elif self.action == 1:\n",
    "            if self.invested > ((self.calc_current_worth()/4)*3):\n",
    "                return -25\n",
    "                # self.done = True\n",
    "            elif self.lastBuy > -60 or self.lastSold > -40:\n",
    "                return -25\n",
    "                # self.done = True\n",
    "            else:\n",
    "                #buy = invest 1/10 of non_invested money\n",
    "                to_invest = self.non_invested / 5\n",
    "                self.non_invested -= to_invest\n",
    "                amount = to_invest / self.current_price[0]\n",
    "                self.invested += to_invest\n",
    "                self.invested_amount += amount\n",
    "                self.lastBuy = 0\n",
    "                self.lastBuyPrice = self.current_price[0]\n",
    "                reward = 1750\n",
    "        elif self.action == 2:\n",
    "            #sell first bought in array of invested\n",
    "            if self.lastBuy > -10 and self.lastSold > -20:\n",
    "                reward = -25\n",
    "            elif self.invested > 10:\n",
    "                amount = self.invested_amount\n",
    "                price_investment = self.invested\n",
    "                self.non_invested += amount * self.current_price[0]\n",
    "                self.invested_amount = 0\n",
    "                self.invested = 0\n",
    "                self.lastSold = 0\n",
    "                self.lastSoldPrice = self.current_price[0]\n",
    "                if ((amount * self.current_price[0]) - price_investment) > 0:\n",
    "                    reward = (((amount * self.current_price[0]) - price_investment))* 500\n",
    "                else:\n",
    "                    reward = (((amount * self.current_price[0]) - price_investment)) * -100\n",
    "            else:\n",
    "                #hold, do nothing, nothing to sell\n",
    "                return -25\n",
    "        \n",
    "        self.worth_history.append(self.calc_current_worth())\n",
    "        if reward > 0: \n",
    "            return (reward) * ((self.calc_current_worth()/self.initial_investment)**2)\n",
    "        else:\n",
    "            return (reward) * ((self.calc_current_worth()/self.initial_investment)**2)\n",
    "\n",
    "\n",
    "\n",
    "    def is_done(self):\n",
    "        #terminal state\n",
    "        if self.calc_current_worth() <= self.Terminal_state:\n",
    "            print(\"terminal state reached\")\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def calc_current_worth(self):\n",
    "        #calculate current worth\n",
    "        return self.invested_amount * self.current_price[0] + self.non_invested\n",
    "\n",
    "        \n",
    "    def csv_to_dataframe(self):\n",
    "        #convert csv's to one dataframe\n",
    "        # df2017 = pd.read_csv('../data/BTC-2017min.csv')\n",
    "        # df2018 = pd.read_csv('../data/BTC-2018min.csv')\n",
    "        # df2019 = pd.read_csv('../data/BTC-2019min.csv')\n",
    "        # df2020 = pd.read_csv('../data/BTC-2020min.csv')\n",
    "        # df2021 = pd.read_csv('../data/BTC-2021min.csv')\n",
    "\n",
    "        #concat all dataframes\n",
    "        # df = pd.concat([df2017, df2018, df2019, df2020, df2021], ignore_index=True)\n",
    "\n",
    "        df = pd.read_csv('./data/ETHUSD_1.csv', header=None)\n",
    "\n",
    "        #add new column named macd, that is the mean of the last 12 open values\n",
    "        #OHLCVT stands for Open, High, Low, Close, Volume and Trades\n",
    "        df['sma60000'] = df[1].rolling(window=60000).mean()\n",
    "        df['sma30000'] = df[1].rolling(window=30000).mean()\n",
    "        #calculate EMA on df\n",
    "        df['ema12days'] = df[4].ewm(span=17280, adjust=False).mean()\n",
    "        df['ema26days'] = df[4].ewm(span=37440, adjust=False).mean()\n",
    "        df['macd'] = df['ema12days'] - df['ema26days']\n",
    "        df.replace([np.inf, -np.inf], np.nan, inplace=True) \n",
    "        df.dropna(inplace=True) \n",
    "        self.df = df\n",
    "        # print(self.df.head())\n",
    "        # print(len(self.df))\n",
    "        #len = 2675301\n",
    "# env = TradingEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfeth = pd.read_csv('./data/ETHUSD_1.csv', header=None)\n",
    "# print(len(dfeth))\n",
    "# dfeth.tail()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Activation, Input\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "import gym, os\n",
    "from gym import wrappers\n",
    "\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "disable_eager_execution()\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.config.experimental_run_functions_eagerly(False)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "import keras\n",
    "class Agent(object):\n",
    "    def __init__(self, alpha, beta, gamma=0.90, n_actions=3,\n",
    "                #  layer1_size=1024, layer2_size=512, input_dims=14):\n",
    "                 layer1_size=1024, layer2_size=512, input_dims=14):\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = layer1_size\n",
    "        self.fc2_dims = layer2_size\n",
    "        self.n_actions = n_actions\n",
    "        self.errorCount = 0\n",
    "        # hier nieuw netwerk maken of oude inladen\n",
    "        self.actor, self.critic, self.policy = self.build_actor_critic_network()\n",
    "        # self.critic = keras.models.load_model('models/MACD/critic')\n",
    "        # self.policy = keras.models.load_model('models/MACD/policy')\n",
    "        # self.actor = load_model('models/MACD/actor')\n",
    "        #unfreeze weights\n",
    "        self.actor.trainable = True\n",
    "        self.critic.trainable = True\n",
    "        self.policy.trainable = True\n",
    "        \n",
    "        \n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.probs = \"\"\n",
    "    # def custom_loss(self,y_true, y_pred):\n",
    "    #     out = K.clip(y_pred, 1e-8, 1-1e-8)\n",
    "    #     log_lik = y_true*K.log(out)\n",
    "    #     delta = Input(shape=[1])\n",
    "    #     return K.sum(-log_lik* Input(shape=[1]))\n",
    "\n",
    "    def build_actor_critic_network(self):\n",
    "        NN_input = Input(shape=(self.input_dims,))\n",
    "        delta = Input(shape=[1])\n",
    "        dense1 = Dense(self.fc1_dims, activation='relu')(NN_input)\n",
    "        dense2 = Dense(self.fc2_dims, activation='relu')(dense1)\n",
    "        probs = Dense(self.n_actions, activation='softmax')(dense2)\n",
    "        values = Dense(1, activation='linear')(dense2)\n",
    "\n",
    "        \n",
    "\n",
    "        actor = Model(inputs=[NN_input, delta], outputs=[probs])\n",
    "\n",
    "        actor.compile(optimizer=Adam(lr=self.alpha), loss='categorical_crossentropy')\n",
    "\n",
    "        critic = Model(inputs=[NN_input], outputs=[values])\n",
    "\n",
    "        critic.compile(optimizer=Adam(lr=self.beta), loss='mean_squared_error')\n",
    "\n",
    "        policy = Model(inputs=[NN_input], outputs=[probs])\n",
    "\n",
    "        return actor, critic, policy\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        try:    \n",
    "            state = observation[np.newaxis, :]\n",
    "            probabilities = self.policy.predict(state)[0]\n",
    "            self.probs = probabilities\n",
    "            action = np.random.choice(self.action_space, p=probabilities)\n",
    "            return action\n",
    "\n",
    "        except Exception as ex:\n",
    "            self.errorCount += 1\n",
    "            print(\"counter\", self.errorCount,\"ex: \", ex)\n",
    "            # print(probabilities)\n",
    "            return 0\n",
    "\n",
    "\n",
    "    def learn(self, state, action, reward, state_, done):\n",
    "        state = state[np.newaxis,:]\n",
    "        state_ = state_[np.newaxis,:]\n",
    "        critic_value_ = self.critic.predict(state_)\n",
    "        critic_value = self.critic.predict(state)\n",
    "\n",
    "        target = reward + self.gamma*critic_value_*(1-int(done))\n",
    "        delta =  target - critic_value\n",
    "\n",
    "        actions = np.zeros([1, self.n_actions])\n",
    "        actions[np.arange(1), action] = 1\n",
    "\n",
    "        self.actor.fit([state, delta], actions, verbose=0)\n",
    "\n",
    "        self.critic.fit(state, target, verbose=0) \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "def train(agent2):\n",
    "    agent = agent2\n",
    "    env = TradingEnv()\n",
    "    score_history = []\n",
    "    \n",
    "    steps = []\n",
    "    actual_steps = []\n",
    "    \n",
    "    num_episodes = 1\n",
    "    fullcounter = 0\n",
    "    maxIterations = 9999999\n",
    "    allEpisodesWorth = []\n",
    "    allEpisodesPrice = []\n",
    "    AllEpisodesSold = []\n",
    "    AllEpisodesBought = []\n",
    "\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        agent.policy.save('models/MACD/policy')\n",
    "        agent.critic.save('models/MACD/critic')\n",
    "        agent.actor.save('models/MACD/actor')\n",
    "        worth_history = []\n",
    "        price_history = []\n",
    "        bought = []\n",
    "        sold = []\n",
    "        done = False\n",
    "        score = 0\n",
    "        observation, done = env.reset()\n",
    "        observation = np.asarray(observation)\n",
    "        counter = 0\n",
    "        lastBuy = -9000\n",
    "\n",
    "        lastSold = -9999\n",
    "        startprice = observation[0]\n",
    "        maxIterations += 5000\n",
    "        while not done:\n",
    "            \n",
    "            counter += 1\n",
    "            fullcounter += 1\n",
    "            # env.render()\n",
    "            action = agent.choose_action(observation)\n",
    "            actual_steps.append(action)\n",
    "            if action == 1 or action == 2:\n",
    "                # print(\"actual action \", action, \"counter: \", counter,\"probs \", agent.probs)\n",
    "                pass\n",
    "            \n",
    "            if np.random.random() < .051:\n",
    "                print(\"random action \", action, \"counter: \", counter,\"probs \", agent.probs)\n",
    "                action = np.random.choice(3, p=[.7,.2,.1])\n",
    "                # print(\"random action \", action, \"counter: \", counter,\"probs \", agent.probs)\n",
    "\n",
    "            # if action == 1:\n",
    "            #     if counter - lastBuy > 250 and counter - lastSold > 100:\n",
    "\n",
    "            #         action = 1\n",
    "            #         lastBuy = counter\n",
    "            #     else:\n",
    "            #         action = 0\n",
    "            # if counter - lastBuy > 10000:\n",
    "            #     action = 1\n",
    "            # if action == 2 and counter - lastSold < 100:\n",
    "            #     if counter - lastBuy > 200:\n",
    "            #         action = 2\n",
    "            #     else:\n",
    "            #         action = 0\n",
    "            # if counter > maxIterations:\n",
    "            #     action = 2\n",
    "            #     lastSold = counter\n",
    "            #     sold.append(counter-1)\n",
    "            # if env.invested > 15000 and action == 1: \n",
    "            #     action = 0\n",
    "            # if env.invested < 100 and action == 2:\n",
    "            #     action = 0\n",
    "            # pick a number between 0 and 1\n",
    "            # if the number is less than epsilon, pick a random action\n",
    "            # if the number is greater than epsilon, pick the action with the highest q value\n",
    "\n",
    "\n",
    "            observation_, reward, done = env.step(action, interval=5)\n",
    "            steps.append(action)\n",
    "            if action == 1 and reward > 0:\n",
    "                lastBuy = counter\n",
    "                bought.append(counter-1)\n",
    "            elif action == 2 and reward != -10000 and reward != -2500:\n",
    "                sold.append(counter-1)\n",
    "            if action == 1 and reward > 0:\n",
    "                # print(\"reward\", reward,\"action: \", action ,\" Episode: \", i, \"Step: \", counter, \"current_worth: \", env.calc_current_worth())\n",
    "                pass\n",
    "            if action == 2 and reward != -10000 and reward != -2500 :\n",
    "                # print(\"reward\", reward,\"action: \", action ,\" Episode: \", i, \"Step: \", counter, \"current_worth: \", env.calc_current_worth())\n",
    "                pass\n",
    "            # if agent.probs[0] < .99:\n",
    "            print(\"episode: \",i,\"counter \", env.counter,\"probas: \", agent.probs, \" action \", action, \"reward: \", reward,  \"invested \", env.invested, \"lastBuy: \", env.lastBuy, \"worth\", env.calc_current_worth() )\n",
    "            observation_ = np.asarray(observation_)\n",
    "            agent.learn(observation, action, reward, observation_, done)\n",
    "            observation = observation_\n",
    "            score += reward\n",
    "            worth_history.append(env.calc_current_worth())\n",
    "            price_history.append(env.current_price)\n",
    "            if counter > maxIterations:\n",
    "                done = True\n",
    "        endprice = observation[0]\n",
    "        \n",
    "        allEpisodesWorth.append(worth_history)\n",
    "        allEpisodesPrice.append(price_history)\n",
    "        AllEpisodesSold.append(sold)\n",
    "        AllEpisodesBought.append(bought)\n",
    "\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "        print('episode: ', i,'worth: %.2f' % env.calc_current_worth(),\n",
    "              'avg score %.2f' % avg_score)\n",
    "        print(\"1buy&sell: startworth:\", 20000, \"endworth:\", (20000/startprice)*endprice)\n",
    "\n",
    "    # save model progress\n",
    "    agent.policy.save('models/MACD/policy')\n",
    "    agent.critic.save('models/MACD/critic')\n",
    "    agent.actor.save('models/MACD/actor')\n",
    "    #make axes for 4 plots under eachother\n",
    "    # print(\"allEpisodesWorth\", len(allEpisodesWorth[0]))\n",
    "    # print(\"allEpisodesPrice\", len(allEpisodesPrice[0]))\n",
    "    # print(\"AllEpisodesSold\", len(AllEpisodesSold[0]))\n",
    "    # print(\"AllEpisodesBought\", len(AllEpisodesBought[0]))\n",
    "    # print(\"startprice\", startprice)\n",
    "    # print(\"endprice\", endprice)\n",
    "\n",
    "    fig, axs = plt.subplots(len(allEpisodesWorth)+2, 1, figsize=(35, 50))\n",
    "    #plot the price on first axis\n",
    "    plot = sns.countplot(steps, ax=axs[0])\n",
    "    plot.bar_label(plot.containers[0])\n",
    "    plot2 = sns.countplot(actual_steps, ax=axs[1])\n",
    "    plot2.bar_label(plot.containers[0])\n",
    "    #set plotje on first axis\n",
    "    # sns.lineplot(price_history,ax=axs[3], color=\"red\", label=\"price\")\n",
    "    # sns.lineplot(worth_history,ax=axs[3], color=\"blue\", label=\"investment_worth\")\n",
    "\n",
    "\n",
    "    # axs[1].plot(price_history)\n",
    "\n",
    "\n",
    "    # axs[1].plot(price_history, color=\"red\", label=\"price\")\n",
    "    # axs[1].plot(worth_history, color=\"blue\", label=\"investment_worth\")\n",
    "    # #print shapes of bought and sold worth_history and price_history\n",
    "    # if len(bought) > 0:\n",
    "    #     axs[1].plot(bought, [worth_history[i] for i in bought], 'o', color='green')\n",
    "    #     axs[1].plot(bought, [price_history[i] for i in bought], 'o', color='green', label=\"bought\")\n",
    "    # if len(sold) > 0:\n",
    "    #     axs[1].plot(sold, [worth_history[i] for i in sold], 'o', color='pink')\n",
    "    #     axs[1].plot(sold, [price_history[i] for i in sold], 'o', color='pink', label=\"sold\")\n",
    "\n",
    "    # axs[1].legend()\n",
    "    # axs[1].set_title(\"Price and worth over time\")\n",
    "    # axs[1].set_xlabel(\"Time\")\n",
    "    # axs[1].set_ylabel(\"Price/Worth\")\n",
    "    #foreach episode plot the worth and price in one plot \n",
    "    for i in range(len(allEpisodesWorth)):\n",
    "        axs[i+2].plot(allEpisodesWorth[i], color=\"blue\", label=\"investment_worth\")\n",
    "        axs[i+2].plot(allEpisodesPrice[i], color=\"red\", label=\"price\")\n",
    "        #print shapes of bought and sold worth_history and price_history\n",
    "        try:\n",
    "            if len(AllEpisodesBought[i]) > 0:\n",
    "                axs[i+2].plot(AllEpisodesBought[i], [allEpisodesWorth[i][j] for j in AllEpisodesBought[i]], 'o', color='green')\n",
    "                axs[i+2].plot(AllEpisodesBought[i], [allEpisodesPrice[i][j] for j in AllEpisodesBought[i]], 'o', color='green', label=\"bought\")\n",
    "            else:\n",
    "                print(\"no bought\")\n",
    "            if len(AllEpisodesSold[i]) > 0:\n",
    "                axs[i+2].plot(AllEpisodesSold[i], [allEpisodesWorth[i][j] for j in AllEpisodesSold[i]], 'o', color='purple')\n",
    "                axs[i+2].plot(AllEpisodesSold[i], [allEpisodesPrice[i][j] for j in AllEpisodesSold[i]], 'o', color='purple', label=\"sold\")\n",
    "            else:\n",
    "                print(\"no sold\")\n",
    "        except Exception as ex:\n",
    "            print(\"error\")\n",
    "            print(ex)\n",
    "            print(AllEpisodesBought[i])\n",
    "            print(AllEpisodesSold[i])\n",
    "            print(len(allEpisodesWorth[i]))\n",
    "            print(len(allEpisodesPrice[i]))\n",
    "            print(allEpisodesWorth[i])\n",
    "            print(allEpisodesPrice[i])\n",
    "            print(\"end error\")\n",
    "            break\n",
    "\n",
    "        axs[i+2].legend()\n",
    "        axs[i+2].set_title(f\"ep: {i} Price and worth over time\")\n",
    "        axs[i+2].set_xlabel(\"Time 1interval = 5min\")\n",
    "        axs[i+2].set_ylabel(\"Price/Worth USD\")\n",
    "\n",
    "       \n",
    "\n",
    "    fig.show()\n",
    "\n",
    "    return agent\n",
    "        \n",
    "\n",
    "      \n",
    "def demo(agent2):\n",
    "    agent = agent2\n",
    "\n",
    "    env = TradingEnv()\n",
    "    score_history = []\n",
    "    \n",
    "    steps = []\n",
    "    \n",
    "    num_episodes = 1\n",
    "    fullcounter = 0\n",
    "    maxIterations = (262975 / 5) - 100\n",
    "\n",
    "    allEpisodesWorth = []\n",
    "    allEpisodesPrice = []\n",
    "    AllEpisodesSold = []\n",
    "    AllEpisodesBought = []\n",
    "\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        # agent.policy.save('models/MACD/policy')\n",
    "        # agent.critic.save('models/MACD/critic')\n",
    "        # agent.actor.save('models/MACD/actor')\n",
    "        worth_history = []\n",
    "        price_history = []\n",
    "        unix_timestamps = []\n",
    "        bought = []\n",
    "        sold = []\n",
    "        done = False\n",
    "        score = 0\n",
    "        observation, done = env.reset(demo=True)\n",
    "        observation = np.asarray(observation)\n",
    "        counter = 0\n",
    "        lastBuy = -9000\n",
    "        lastSold = -9999\n",
    "        startprice = observation[0]\n",
    "        while not done:\n",
    "            unix_timestamps.append(env.current_unix_time)\n",
    "            counter += 1\n",
    "            fullcounter += 1\n",
    "            action = agent.choose_action(observation)\n",
    "            if np.random.random() < .005:\n",
    "                action = np.random.choice(3, p=[.6,.3,.1])\n",
    "            # if np.random.random() < .005:\n",
    "            #     action = np.random.randint(0,3)\n",
    "            \n",
    "            \n",
    "            steps.append(action)\n",
    "\n",
    "\n",
    "            observation_, reward, done = env.step(action, interval=5)\n",
    "            if action == 1 and reward >= 0:\n",
    "                bought.append(counter-1)\n",
    "            elif action == 2:\n",
    "                sold.append(counter-1)\n",
    "            if action == 2:\n",
    "                # print(\"reward\", reward,\"action: \", action ,\" Episode: \", i, \"Step: \", counter, \"current_worth: \", env.calc_current_worth())\n",
    "                pass\n",
    "                \n",
    "            observation_ = np.asarray(observation_)\n",
    "            agent.learn(observation, action, reward, observation_, done)\n",
    "            observation = observation_\n",
    "            score += reward\n",
    "            worth_history.append(env.calc_current_worth())\n",
    "            price_history.append(env.current_price)\n",
    "            if counter > maxIterations:\n",
    "                done = True\n",
    "            print(\"episode: \",i,\"counter \", env.counter,\"probas: \", agent.probs, \" action \", action, \"reward: \", reward,  \"invested \", env.invested, \"lastBuy: \", env.lastBuy, \"worth\", env.calc_current_worth() )\n",
    "            \n",
    "        endprice = observation[0]\n",
    "        allEpisodesWorth.append(worth_history)\n",
    "        allEpisodesPrice.append(price_history)\n",
    "        AllEpisodesSold.append(sold)\n",
    "        AllEpisodesBought.append(bought)\n",
    "\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "        print('episode: ', i,'worth: %.2f' % env.calc_current_worth(),\n",
    "              'avg score %.2f' % avg_score)\n",
    "        print(\"1buy&sell: startworth:\", 20000, \"endworth:\", (20000/startprice)*endprice)\n",
    "\n",
    "    # # save model progress\n",
    "    # agent.policy.save('models/MACD/policy')\n",
    "    # agent.critic.save('models/MACD/critic')\n",
    "    # agent.actor.save('models/MACD/actor')\n",
    "    #make axes for 4 plots under eachother\n",
    "    # print(\"allEpisodesWorth\", len(allEpisodesWorth[0]))\n",
    "    # print(\"allEpisodesPrice\", len(allEpisodesPrice[0]))\n",
    "    # print(\"AllEpisodesSold\", len(AllEpisodesSold[0]))\n",
    "    # print(\"AllEpisodesBought\", len(AllEpisodesBought[0]))\n",
    "    # print(\"startprice\", startprice)\n",
    "    # print(\"endprice\", endprice)\n",
    "\n",
    "    fig, axs = plt.subplots(len(allEpisodesWorth)+1, 1, figsize=(35, 50))\n",
    "    #plot the price on first axis\n",
    "    plot = sns.countplot(steps, ax=axs[0])\n",
    "    plot.bar_label(plot.containers[0])\n",
    "    #set plotje on first axis\n",
    "    # sns.lineplot(price_history,ax=axs[3], color=\"red\", label=\"price\")\n",
    "    # sns.lineplot(worth_history,ax=axs[3], color=\"blue\", label=\"investment_worth\")\n",
    "\n",
    "\n",
    "    # axs[1].plot(price_history)\n",
    "\n",
    "\n",
    "    # axs[1].plot(price_history, color=\"red\", label=\"price\")\n",
    "    # axs[1].plot(worth_history, color=\"blue\", label=\"investment_worth\")\n",
    "    # #print shapes of bought and sold worth_history and price_history\n",
    "    # if len(bought) > 0:\n",
    "    #     axs[1].plot(bought, [worth_history[i] for i in bought], 'o', color='green')\n",
    "    #     axs[1].plot(bought, [price_history[i] for i in bought], 'o', color='green', label=\"bought\")\n",
    "    # if len(sold) > 0:\n",
    "    #     axs[1].plot(sold, [worth_history[i] for i in sold], 'o', color='pink')\n",
    "    #     axs[1].plot(sold, [price_history[i] for i in sold], 'o', color='pink', label=\"sold\")\n",
    "\n",
    "    # axs[1].legend()\n",
    "    # axs[1].set_title(\"Price and worth over time\")\n",
    "    # axs[1].set_xlabel(\"Time\")\n",
    "    # axs[1].set_ylabel(\"Price/Worth\")\n",
    "    #foreach episode plot the worth and price in one plot \n",
    "    for i in range(len(allEpisodesWorth)):\n",
    "        axs[i+1].plot(allEpisodesWorth[i], color=\"blue\", label=\"investment_worth\")\n",
    "        axs[i+1].plot(allEpisodesPrice[i], color=\"red\", label=\"price\")\n",
    "        #print shapes of bought and sold worth_history and price_history\n",
    "        try:\n",
    "            if len(AllEpisodesBought[i]) > 0:\n",
    "                axs[i+1].plot(AllEpisodesBought[i], [allEpisodesWorth[i][j] for j in AllEpisodesBought[i]], 'o', color='green')\n",
    "                axs[i+1].plot(AllEpisodesBought[i], [allEpisodesPrice[i][j] for j in AllEpisodesBought[i]], 'o', color='green', label=\"bought\")\n",
    "            else:\n",
    "                print(\"no bought\")\n",
    "            if len(AllEpisodesSold[i]) > 0:\n",
    "                axs[i+1].plot(AllEpisodesSold[i], [allEpisodesWorth[i][j] for j in AllEpisodesSold[i]], 'o', color='purple')\n",
    "                axs[i+1].plot(AllEpisodesSold[i], [allEpisodesPrice[i][j] for j in AllEpisodesSold[i]], 'o', color='purple', label=\"sold\")\n",
    "            else:\n",
    "                print(\"no sold\")\n",
    "        except Exception as ex:\n",
    "            print(\"error\")\n",
    "            print(ex)\n",
    "            print(AllEpisodesBought[i])\n",
    "            print(AllEpisodesSold[i])\n",
    "            print(len(allEpisodesWorth[i]))\n",
    "            print(len(allEpisodesPrice[i]))\n",
    "            print(allEpisodesWorth[i])\n",
    "            print(allEpisodesPrice[i])\n",
    "            print(\"end error\")\n",
    "            break\n",
    "        import datetime\n",
    "        axs[i+1].legend()\n",
    "        axs[i+1].set_xticklabels([datetime.datetime.fromtimestamp(tm) for tm in unix_timestamps],\n",
    "        rotation=50)\n",
    "        axs[i+1].set_title(f\"ep: {i} Price and worth over time\")\n",
    "        axs[i+1].set_xlabel(\"Time 1interval = 5min\")\n",
    "        axs[i+1].set_ylabel(\"Price/Worth USD\")\n",
    "\n",
    "       \n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "# agent = Agent(alpha=0.001, beta=0.005)\n",
    "agent = train(agent)\n",
    "agent = train(agent)\n",
    "agent = train(agent)\n",
    "agent = train(agent)\n",
    "agent = train(agent)\n",
    "agent = train(agent)\n",
    "agent = train(agent)\n",
    "agent = train(agent)\n",
    "\n",
    "\n",
    "\n",
    "# agent = train(agent)\n",
    "# # save model progress\n",
    "agent.policy.save_weights('models/weights/MACD3/policy')\n",
    "agent.critic.save_weights('models/weights/MACD3/critic')\n",
    "agent.actor.save_weights('models/weights/MACD3/actor')\n",
    "\n",
    "# demo(agent)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ResearchProj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12 (default, Oct 12 2021, 03:01:40) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eff080227ac678accb1f47f22e9c228faa9b0afa218e8070dc42d5c73fc603f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
